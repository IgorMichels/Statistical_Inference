\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[portuges]{babel}
\usepackage{csquotes}
\usepackage{geometry}
\usepackage[pdftex]{hyperref}
\usepackage{indentfirst}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{mathrsfs}
\usepackage{graphicx}
\usepackage{float}
\usepackage{multicol}

\usepackage{pgfplots}
\usepackage{tikz}
\usepackage{xcolor}
\pgfplotsset{compat = 1.16}
\pgfmathdeclarefunction{gauss}{2}{\pgfmathparse{1/(#2*sqrt(2*pi))*exp(-((x-#1)^2)/(2*#2^2))}}
\pgfmathdeclarefunction{gamma}{1}{\pgfmathparse{2.506628274631*sqrt(1/#1)+0.20888568*(1/#1)^(1.5)+
    0.00870357*(1/#1)^(2.5)-(174.2106599*(1/#1)^(3.5))/25920-
    (715.6423511*(1/#1)^(4.5))/1244160)*exp((-ln(1/#1)-1)*#1)}}
\pgfmathdeclarefunction{studentpost}{1}{\pgfmathparse{gamma((#1+1)/2)/(sqrt(#1*pi)*gamma(#1/2))*((1+((x-9.251579)*(x-9.251579))/#1)^(-(#1+1)/2))}}
\pgfmathdeclarefunction{studentprio}{1}{\pgfmathparse{gamma((#1+1)/2)/(sqrt(#1*pi)*gamma(#1/2))*((1+((x-10)*(x-10))/#1)^(-(#1+1)/2))}}

\newtheorem{definition}{Definição}
\newtheorem{theorem}{Teorema}
\newtheorem{lemma}[theorem]{Lema}
\newtheorem{example}{Exemplo}

\usepackage[backend = biber]{biblatex}
\addbibresource{terceiro_trabalho.bib}

\geometry{left = 3cm, top = 3cm, bottom = 2cm, right = 2cm}

\title{Inferência Estatística \\ 3º Trabalho}
\author{Igor Patrício Michels}
\date{21/10/2020}

\begin{document}

\maketitle

\section*{Introdução}

Trabalho elaborado pelo aluno Igor Patrício Michels referente a disciplina de Inferência Estatística, do quarto período da Graduação em Matemática Aplicada da FGV-EMAp. Nele faremos uma análise bayesiana da distribuição normal.

O enunciado e eventuais funções utilizadas para resolução deste ou de outros trabalhos podem ser encontrados \href{https://github.com/IgorMichels/Statistical_Inference}{\textbf{nesse repositório do GitHub}}. Já o presente relatório em \LaTeX{} pode ser encontrado \href{https://www.overleaf.com/read/sdfnbcnthrms}{\textbf{nesse link}}.

\section*{Uma Análise Bayesiana no Caso Normal}

\subsection*{Precisão}

Antes de começar nossa análise vamos definir o termo precisão, bem como dar uma pequena intuição de seu significado.

\begin{definition}
    Seja $\sigma^2$ a variância de uma distribuição. O parâmetro $\tau = \left(\sigma^2\right)^{-1}$ é chamado de precisão.
\end{definition}

Note que a precisão é definida como o recíproco da variância, uma medida de dispersão dos dados. Assim, se possuímos uma variância baixa afirmar ver que os valores tenderam a estarem próximos da média da distribuição. Já se a variância é alta os valores poderão estar mais espalhados ao redor da média, como ilustrado na figura \ref{gaussianas}.
\begin{figure}[H]
    \begin{tikzpicture}
        \begin{axis}[no markers, domain = 0:10, samples = 100, height = 5 cm, width = 8 cm, enlargelimits = upper, axis on top]
            \addplot [fill = cyan!20, draw = none, domain = -3:3] {gauss(0,1)} \closedcycle;
            \addplot [fill = orange!20, draw = none, domain = -3:-2] {gauss(0,1)} \closedcycle;
            \addplot [fill = orange!20, draw = none, domain = 2:3] {gauss(0,1)} \closedcycle;
            \addplot [fill = blue!20, draw = none, domain = -2:-1] {gauss(0,1)} \closedcycle;
            \addplot [fill = blue!20, draw = none, domain = 1:2] {gauss(0,1)} \closedcycle;
        \end{axis}
    \end{tikzpicture}
    \begin{tikzpicture}
        \begin{axis}[no markers, domain = 0:10, samples = 100, height = 5 cm, width = 8 cm, enlargelimits = upper, axis on top]
            \addplot [fill = cyan!20, draw = none, domain = -6:6] {gauss(0,2)} \closedcycle;
            \addplot [fill = orange!20, draw = none, domain = -6:-4] {gauss(0,2)} \closedcycle;
            \addplot [fill = orange!20, draw = none, domain = 4:6] {gauss(0,2)} \closedcycle;
            \addplot [fill = blue!20, draw = none, domain = -4:-2] {gauss(0,2)} \closedcycle;
            \addplot [fill = blue!20, draw = none, domain = 2:4] {gauss(0,2)} \closedcycle;
        \end{axis}
    \end{tikzpicture}
    \caption{à esquerda temos a plotagem de uma $\mathcal{N}(0, 1^2)$, já à direita a plotagem de uma $\mathcal{N}(0, 2^2)$.}
    \label{gaussianas}
\end{figure}

Note que na figura \ref{gaussianas} ambas as p.d.f.'s são iguais, com exceção a escala, a qual foi modificada em virtude da mudança na variância. Esse detalhe nos possibilita perceber a regra 68-95-99.7\footnote{A regra 68-95-99.7 é a propriedade de que aproximadamente 68\% dos valores estão a menos de um desvio padrão da média da normal, bem como 95\% e 99.7\% estão a uma distância inferior a dois e três desvios padrão, respectivamente.} da normal, além de reforçar a ideia de que, quanto maior a variância, maior a dispersão dos valores em torno da média.\footnote{Aqui poderíamos apenas citar a regra 96-95-99.7, entretanto a plotagem da distribuição da normal facilita o entendimento, uma vez que a forma da curva permanece a mesma, mas a escala do eixo $x$ aumenta.}

Dessa forma, a precisão pode ser considerada uma medida de proximidade dos dados, pois a precisão é definida como o recíproco da variância. Assim, quando temos uma variância baixa os valores se encontram perto da média e a precisão é alta, já quando a variância é grande os valores estão mais dispersos e a precisão é menor.

Definida a ideia de precisão podemos nos perguntar qual a utilidade da mesma além de ser uma métrica como citado anteriormente. Podemos responder a essa dúvida mostrando qual a função de densidade de probabilidade de uma variável com distribuição normal em cada uma das parametrizações:
\[X \sim \mathcal{N}(\mu, \sigma^2) \longrightarrow f_X(x \mid \mu, \tau) = \dfrac{1}{\sqrt{2\pi \sigma^2}}\exp{\left[-\dfrac{1}{2}\left(\dfrac{x - \mu}{\sigma}\right)^2\right]}\]
\[X \sim \mathcal{N}_2(\mu, \tau) \longrightarrow f_X(x \mid \mu, \tau) = \sqrt{\dfrac{\tau}{2\pi}}\exp{\left[-\dfrac{1}{2}\tau(x - \mu)^2\right]},\]

\noindent onde $\mathcal{N}_2$ representa a normal parametrizada com a média e precisão. Já para uma distribuição conjunta temos as seguintes parametrizações:
\begin{itemize}
    \item
        se $X_1, \dots, X_n \sim \mathcal{N}(\mu, \sigma^2)$:
        \begin{equation*}
            \begin{split}
                f_X(x_1, \dots, x_n \mid \mu, \tau) & = \prod_{i = 1}^{n} \dfrac{1}{\sqrt{2\pi \sigma^2}}\exp{\left[-\dfrac{1}{2}\left(\dfrac{x_i - \mu}{\sigma}\right)^2\right]} \\
                & = \left(\dfrac{1}{\sqrt{2\pi \sigma^2}}\right)^n \prod_{i = 1}^{n} \exp{\left[-\dfrac{1}{2}\left(\dfrac{x_i - \mu}{\sigma}\right)^2\right]} \\
                & = \left(\sqrt{2\pi \sigma^2}\right)^{-\frac{n}{2}} \exp{\left[-\dfrac{1}{2\sigma^2}\sum_{i = 1}^{n}\left(x_i - \mu\right)^2\right]} \\
            \end{split}
        \end{equation*}
        
    \item
        já se $X_1, \dots, X_n \sim \mathcal{N}_2(\mu, \tau)$:
        \begin{equation}
            \label{eq1}
            \begin{split}
                f_X(x_1, \dots, x_n \mid \mu, \tau) & = \prod_{i = 1}^{n} \sqrt{\dfrac{\tau}{2\pi}}\exp{\left[-\dfrac{1}{2}\tau(x_i - \mu)^2\right]} \\
                & = \left(\sqrt{\dfrac{\tau}{2\pi}}\right)^n \prod_{i = 1}^{n} \exp{\left[-\dfrac{1}{2}\tau\left(x_i - \mu\right)^2\right]} \\
                & = \left(\sqrt{\dfrac{\tau}{2\pi}}\right)^n \exp{\left[-\dfrac{1}{2}\tau\sum_{i = 1}^{n}\left(x_i - \mu\right)^2\right]} \\
            \end{split}
        \end{equation}
\end{itemize}

Perceba que, nos dois casos, a segunda representação é mais amigável, uma vez que é mais fácil de manipular os parâmetros manualmente pois não ocorrem divisões por um parâmetro, o que pode ser complicado. Além disso, com essa segunda representação podemos perceber facilmente pequenas variações \cite{stackexchange}.

\subsection*{Inferência utilizando a Precisão}

Dada a distribuição conjunta condicional dos dados em \ref{eq1}, podemos buscar uma distribuição a priori para $\mu$ e $\tau$. Assim como em \cite{ehlers} e \cite{ehlers2}, vamos reescrever $P(\mu, \tau) = P(\mu \mid \tau)\cdot P(\tau)$, dessa forma, vamos encontrar uma priori em duas etapas. Na primeira etapa, note que, dado $\tau$, a distribuição conjunta tem a forma de uma normal, assim escrevemos
\[\mu \mid \tau \sim \mathcal{N}_2(\mu_0, \lambda_0 \tau).\]

Já para a segunda etapa, quando precisamos encontrar a priori de $\tau$, podemos notar que, do ponto de vista de $\tau$, o núcleo da distribuição em \ref{eq1} tem a mesma forma que o núcleo de uma distribuição Gama. Dessa forma, temos que
\[\tau \sim \text{Gama}\left(\alpha_0, \beta_0\right).\]

Já para o cálculo da posteriori, podemos seguir uma ideia similar a que DeGroot fez em \cite{degroot}. Sejam $\xi_1$ e $\xi_2$ as prioris de $\mu \mid \tau$ e $\tau$, respectivamente. Assim, temos que
\[\xi_1(\mu \mid \tau) \propto \sqrt{\tau} \exp{\left[-\dfrac{1}{2}\lambda_0 \tau (\mu - \mu_0)^2\right]} \text{ e que } \xi_2(\tau) \propto \tau^{\alpha_0 - 1} \exp{\left[- \beta_0 \tau\right]}.\]

Dessa forma, a posteriori da distribuição conjunta deve satisfazer
\begin{equation}
    \label{post1}
    \begin{split}
        \xi(\mu, \tau \mid x_1, x_2, \dots, x_n) & \propto f_X(x_1, \dots, x_n \mid \mu, \tau) \xi_1(\mu \mid \tau) \xi_2(\tau) \\
        & \propto \tau^{\alpha_0 - 1 + (n + 1)/2} \exp{\left[-\dfrac{\tau}{2}\left(\lambda_0 (\mu - \mu_0)^2 + \sum_{i = 1}^{n}\left(x_i - \mu\right)^2\right) - \beta_0 \tau\right]}.
    \end{split}
\end{equation}

Agora, perceba que
\begin{equation*}
    \begin{split}
        \sum_{i = 1}^{n} \left(x_i - \mu\right)^2 & = \sum_{i = 1}^{n} \left(x_i - \overline{x}_n + \overline{x}_n - \mu\right)^2 \\
        & = \sum_{i = 1}^{n} \left[\left(x_i - \overline{x}_n\right)^2 + 2\left(x_i - \overline{x}_n\right)\left(\overline{x}_n - \mu\right) + \left(\overline{x}_n - \mu\right)^2\right] \\
        & = \sum_{i = 1}^{n} \left(x_i - \overline{x}_n\right)^2 + \sum_{i = 1}^{n} 2\left(x_i - \overline{x}_n\right)\left(\overline{x}_n - \mu\right) + \sum_{i = 1}^{n} \left(\overline{x}_n - \mu\right)^2 \\
        & = \overline{s}_n^2 + 2\left(\overline{x}_n - \mu\right)\sum_{i = 1}^{n} \left(x_i - \overline{x}_n\right) + n\left(\overline{x}_n - \mu\right)^2 \\
        & = \overline{s}_n^2 + 2\left(\overline{x}_n - \mu\right)\cdot 0 + n\left(\overline{x}_n - \mu\right)^2 \\
        & = \overline{s}_n^2 + n\left(\overline{x}_n - \mu\right)^2.
    \end{split}
\end{equation*}

Assim, podemos reescrever \ref{post1} como
\begin{equation}
    \label{post2}
    \xi(\mu, \tau \mid x_1, x_2, \dots, x_n) \propto \tau^{\alpha_0 - 1 + (n + 1)/2} \exp{\left[-\dfrac{\tau}{2}\left(\lambda_0 (\mu - \mu_0)^2 + \overline{s}_n^2 + n\left(\overline{x}_n - \mu\right)^2\right) - \beta_0 \tau\right]}.
\end{equation}

Note também que
\begin{equation}
    \label{ig}
    \begin{split}
        n\left(\overline{x}_n - \mu\right)^2 + \lambda_0\left(\mu - \mu_0\right)^2 & = n\overline{x}_n^2 - 2n\overline{x}_n\mu + n\mu^2 + \lambda_0\mu^2 - 2\lambda_0\mu\mu_0 + \lambda_0\mu_0^2 \\
        & = (n + \lambda_0)\left(\mu^2 - \dfrac{2n\overline{x}_n\mu + 2\lambda_0\mu\mu_0}{n + \lambda_0} + \dfrac{\left(n + \lambda_0\right)\left(n\overline{x}_n^2 + \lambda_0\mu_0^2\right)}{\left(n + \lambda_0\right)^2}\right),
    \end{split}
\end{equation}

\noindent desenvolvendo a última fração de \ref{ig} separadamente, temos
\begin{equation*}
    \begin{split}
        \dfrac{\left(n + \lambda_0\right)\left(n\overline{x}_n^2 + \lambda_0\mu_0^2\right)}{\left(n + \lambda_0\right)^2} & = \dfrac{n^2\overline{x}_n^2 + n\lambda_0\mu_0^2 + n\lambda_0\overline{x}_n^2 + \lambda_0^2\mu_0^2}{\left(n + \lambda_0\right)^2} \\
        & = \dfrac{\lambda_0^2\mu_0^2 + 2\mu_0\lambda_0n\overline{x}_n + n^2\overline{x}_n^2 + n\lambda_0\overline{x}_n^2 - 2\mu_0\lambda_0n\overline{x}_n + n\lambda_0\mu_0^2}{\left(n + \lambda_0\right)^2} \\
        & = \dfrac{\left(\lambda_0^2\mu_0^2 + 2\mu_0\lambda_0n\overline{x}_n + n^2\overline{x}_n^2\right) + n\lambda_0 \left(\overline{x}_n^2 - 2\mu_0\overline{x}_n + \mu_0^2\right)}{\left(n + \lambda_0\right)^2} \\
        & = \dfrac{\left(\lambda_0\mu_0 + n\overline{x}_n\right)^2 + n\lambda_0 \left(\overline{x}_n - \mu_0\right)^2}{\left(n + \lambda_0\right)^2}.
    \end{split}
\end{equation*}

Substituindo isso em \ref{ig}, temos
\begin{equation*}
    \begin{split}
        n\left(\overline{x}_n - \mu\right)^2 + \lambda_0\left(\mu - \mu_0\right)^2 & = (n + \lambda_0)\left(\mu^2 - \dfrac{2n\overline{x}_n\mu + 2\lambda_0\mu\mu_0}{n + \lambda_0} + \dfrac{\left(\lambda_0\mu_0 + n\overline{x}_n\right)^2 +  n\lambda_0 \left(\overline{x}_n - \mu_0\right)^2}{\left(n + \lambda_0\right)^2}\right) \\
        & = (n + \lambda_0)\left(\mu^2 - \dfrac{2\mu \left(n\overline{x}_n + \lambda_0\mu_0\right)}{n + \lambda_0} + \dfrac{\left(\lambda_0\mu_0 + n\overline{x}_n\right)^2}{\left(n + \lambda_0\right)^2}\right) + \dfrac{n\lambda_0 \left(\overline{x}_n - \mu_0\right)^2}{n + \lambda_0} \\
        & = (n + \lambda_0)\left(\mu - \dfrac{\lambda_0\mu_0 + n\overline{x}_n}{n + \lambda_0}\right)^2 + \dfrac{n\lambda_0 \left(\overline{x}_n - \mu_0\right)^2}{n + \lambda_0}.
    \end{split}
\end{equation*}

Dessa forma, substituindo o resultado acima em \ref{post2}, obtemos
\begin{equation*}
    \xi(\mu, \tau \mid x_1, x_2, \dots, x_n) \propto \tau^{\alpha_0 - 1 + (n + 1)/2} \exp{\left[-\dfrac{\tau}{2}\left(\overline{s}_n^2 + (n + \lambda_0)\left(\mu - \dfrac{\lambda_0\mu_0 + n\overline{x}_n}{n + \lambda_0}\right)^2 + \dfrac{n\lambda_0 \left(\overline{x}_n - \mu_0\right)^2}{n + \lambda_0}\right) - \beta_0 \tau\right]}.
\end{equation*}

Definindo
\begin{equation*}
    \begin{split}
        \lambda_1 & = \lambda_0 + n \\
        \mu_1 & = \dfrac{\lambda_0\mu_0 + n\overline{x}_n}{n + \lambda_0} \\
        \alpha_1 & = \alpha_0 + \dfrac{n}{2} \\
        \beta_1 & = \beta_0 + \dfrac{\overline{s}_n^2}{2} + \dfrac{n\lambda_0\left(\overline{x}_n - \mu_0\right)^2}{2\left(\lambda_0 + n\right)},
    \end{split}
\end{equation*}
% \[\lambda_1 = \lambda_0 + n \text{, } \mu_1 = \dfrac{\lambda_0\mu_0 + n\overline{x}_n}{n + \lambda_0} \text{, } \alpha_1 = \alpha_0 + \dfrac{n}{2} \text{ e } \beta_1 = \beta_0 + \dfrac{s_n^2}{2} + \dfrac{n\lambda_0\left(\overline{x}_n^2 - \mu_0\right)^2}{2\left(\lambda_0 + n\right)},\]

\noindent podemos escrever o termo dentro da exponencial como\footnote{Como as expressões são muito grandes, estou manipulando as mesmas de uma linha para outra.}
\[-\dfrac{\tau}{2}\left(\overline{s}_n^2 + (n + \lambda_0)\left(\mu - \dfrac{\lambda_0\mu_0 + n\overline{x}_n}{n + \lambda_0}\right)^2 + \dfrac{n\lambda_0 \left(\overline{x}_n - \mu_0\right)^2}{n + \lambda_0}\right) - \beta_0 \tau\]
\[-\dfrac{1}{2}\left(\lambda_0 + n\right)\tau\left(\mu - \left(\dfrac{\lambda_0\mu_0 + n\overline{x}_n}{n + \lambda_0}\right)\right)^2 - \tau\left(\beta_0 + \dfrac{\overline{s}_n^2}{2} + \dfrac{n\lambda_0 \left(\overline{x}_n - \mu_0\right)^2}{2\left(n + \lambda_0\right)}\right)\]
\[-\dfrac{1}{2}\lambda_1\tau\left(\mu - \mu_1\right)^2 - \tau\beta_1.\]

Já o expoente de $\tau$ pode ser escrito como
\begin{equation*}
    \begin{split}
        \alpha_0 - 1 + \dfrac{n + 1}{2} & = \alpha_0 + \dfrac{n}{2} - 1 + \dfrac{1}{2} \\
        & = \alpha_1 - 1 + \dfrac{1}{2}.
    \end{split}
\end{equation*}

Dessa forma, temos

\begin{equation*}
    \begin{split}
        \xi(\mu, \tau \mid x_1, x_2, \dots, x_n) & \propto \tau^{\alpha_1 - 1 + 1/2} \exp{\left[-\dfrac{1}{2}\lambda_1\tau\left(\mu - \mu_1\right)^2 -\beta_1\tau\right]} \\
        & = \tau^{1/2} \exp{\left[-\dfrac{1}{2}\lambda_1\tau\left(\mu - \mu_1\right)^2\right]} \cdot \tau^{\alpha_1 - 1} \exp{\left[-\beta_1\tau\right]}.
    \end{split}
\end{equation*}

Assim, obtemos a distribuição conjunta a posteriori de $\mu$ e $\tau$. Além disso, o termo
\[\tau^{1/2} \exp{\left[-\dfrac{1}{2}\lambda_1\tau\left(\mu - \mu_1\right)^2\right]},\]

\noindent é, exceto por um fator que independe de $\mu$ e $\tau$, a distribuição a posteriori de $\mu \mid \tau$, sendo da forma Normal$_2$ com média $\mu_1$ e precisão $\lambda_1\tau$. Já o termo
\[\tau^{\alpha_1 - 1} \exp{\left[-\beta_1\tau\right]}\]

\noindent é, também exceto por um fator multiplicativo que independe de $\mu$ e $\tau$, a distribuição a posteriori de $\tau$. Note que essa distribuição é uma Gama de parâmetros $\alpha_1$ e $\beta_1$.

Esse conjunto de prioris utilizadas acima para esse caso normal em que desconhecemos tanto a média quanto a precisão é conhecida como Distribuição Normal-Gama. Note que essa distribuição é fechada no cálculo da posteriori, ou seja, a posteriori tem a mesma distribuição da priori. Já a distribuição marginal de $\mu$ a posteriori pode ser encontrada integrando o produto de $P(\tau)$ com $P(\mu \mid \tau)$ no domínio de $\tau$, isto é $\mathbb{R}_{>0}$. Fazendo isto, temos
\begin{equation*}
    \begin{split}
        P(\mu) & = \int_{0}^{\infty} P(\mu \mid \tau) P(\tau) ~d\tau \\
        & \propto \int_{0}^{\infty} \tau^{1/2} \exp{\left[-\dfrac{1}{2}\lambda_1\tau\left(\mu - \mu_1\right)^2\right]} \tau^{\alpha_1 - 1} \exp{\left[-\beta_1\tau\right]} ~d\tau \\
        & \propto \int_{0}^{\infty} \tau^{\alpha_1 - 1 + 1/2} \exp{\left[-\dfrac{1}{2}\lambda_1\tau\left(\mu - \mu_1\right)^2 - \beta_1\tau\right]} ~d\tau \\
        & \propto \left(\dfrac{2\beta_1 + \lambda_1 \left(\mu - \mu_1\right)^2}{2}\right)^{-\left(\alpha_1 + \frac{1}{2}\right)} \\
        & \propto \left(1 + \dfrac{\left(\mu - \mu_1\right)^2}{\frac{2\beta_1}{\lambda_1}}\right)^{-\left(\alpha_1 + \frac{1}{2}\right)},
    \end{split}
\end{equation*}

\noindent onde podemos ver que $\mu$ tem distribuição $t$ deslocada, isto é,
\[\mu \sim X = \mu_1 + \left(\dfrac{\beta_1}{\alpha_1 \lambda_1}\right)^{\frac{1}{2}}T,\]

\noindent com $T$ tendo distribuição $t$ de Student com $2\alpha_1$ graus de liberdade.

\subsubsection*{Paralelo com o Frequentismo}

Fazendo um paralelo com a ideia frequentista, note que a média a posteriori $\left(\mu_1\right)$ é função da média amostral $\left(\overline{x}_n\right)$, que é o EMV, e dos hiperparâmetros iniciais:
\[\mu_1 = \dfrac{\lambda_0\mu_0 + n\overline{x}_n}{n + \lambda_0}.\]

Note que o lado direito da expressão acima pode ser interpretada como uma média ponderada entre $\mu_0$ e $\overline{x}_n$. Dessa forma, $\mu_0$ tem peso $\lambda_0$ e $\overline{x}_n$ tem peso $n$ e, com isso, nota-se que quando o número de amostras cresce $(n \to \infty)$, temos que $\mu_1 \to \overline{x}_n$. Dessa forma, podemos interpretar $\lambda_0$ como se fosse uma espécie de ``métrica de certeza'' ou então uma ``métrica de aversão'' em relação as ideias, a priori, sobre $\mu_0$.

Exemplificando essa ideia, tomando $\mu_0$ e $\lambda_0 > n$ teremos, após a atualização dos hiperparâmetros, um valor de $\mu_1$ mais próximo de $\mu_0$ que de $\overline{x}_n$. Dessa forma, um $\lambda_0$ maior que $n$ implica que, de certo modo, estamos confiando mais nos conhecimentos prévios sobre o experimento que em nossas observações. Podemos notar também que, se $\lambda_0 \gg n$, a diferença entre $\mu_1$ e $\mu_0$ é muito pequena, o que pode nos levar a ideia do parágrafo anterior, ou seja, a de considerar $\lambda_0$ como uma ``métrica de certeza'' ou uma ``métrica de aversão'', assim, quanto maior $\lambda_0$, podemos dizer que estamos mais certos que sobre $\mu$ a partir de nossa ideia $\mu_0$ ou então que estamos mais aversos a alterações na atualização do parâmetro $\mu_0$. Note que, nos dois casos, necessitamos uma grande quantidade de amostras para uma alteração significativa de $\mu_0$ para $\mu_1$, quando a mesma for possível.\footnote{Note que podemos ter $\mu_0$ muito próximo do valor esperado de $\mu$, então ter um $\lambda_0$ alto, indicando certeza ou aversão, acaba não sendo relevante, pois o esperado é que, com $n$ grande, $\overline{x}_n$ também esteja próximo ao valor esperado de $\mu$, logo, as diferença entre $\mu_0$ e $\mu_1$ acaba se tornando pequena, mesmo com $\lambda_0$ pequeno.}

Após termos visto o desenvolvimento das distribuições a posteriori vamos ilustrar essas ideias com um exemplo.

\subsection*{Um pequeno exemplo}

\begin{example}
    Palmirinha anda preocupada com a concentração de amido em sua pamonha. Ela pede para Valciclei, seu assistente, amostrar $n=10$ pamonhas e medir sua concentração de amido.
    
    Ele, muito prestativo, rapidamente faz o experimento, mas, porque comeu todas as amostras depois que foram medidas, precisou fazer uma visita de emergência ao banheiro. Desta feita, apenas teve tempo de anotar em um papel a média e variância amostrais, $\bar{x}_n =  8.307849$ e $\bar{s}^2_n = 7.930452$.
    
    Palmirinha tem uma reunião com investidores em pouco tempo, então decide voltar aos seus tempos de bayesiana~\textit{old school} e analisar os dados utilizando prioris conjugadas. Ela supõe que a concentração de amido segue uma distribuição normal com parâmetros $\mu$ e $\tau$ e que as observações feitas por Valciclei são independentes entre si. Ela suspeita que a concentração de amido na pamonha fique em torno de $10$ mg/L, com desvio padrão de  $2$ mg/L. Com sua larga experiência na confecção de pamonhas, ela suspeita ainda que o coeficiente de variação da concentração de amido seja em torno de $1/2$. Palmirinha tem um quadro em seu escritório, que diz
    \[\operatorname{cv} = \frac{\sigma}{\mu}.\]
    
    Agora, 
    \begin{enumerate}
        \item
            Com os dados anotados por Valciclei, é possível computar a distribuição~\textit{a posteriori} de $\mu$ e $\tau$? Justifique.
            
        \item
            Em caso afirmativo, ajude Palmirinha a encontrar $a, b \in \mathbb{R}$, $a < b$ de modo que $\operatorname{Pr}(\mu \in (a, b) \mid \boldsymbol{x}) = 0.95$.
    \end{enumerate}
\end{example}

Nesse exemplo, apesar de parecer que não é possível encontrar a distribuição a posteriori de $\mu$ e $\tau$, é possível sim, conforme visto anteriormente, encontrar tal distribuição.

Primeiramente, note que, dados $\lambda_0$, $\mu_0$, $\alpha_0$ e $\beta_0$, necessitamos apenas de $n$, $\overline{x}_n$ e $\overline{s}_n^2$ para computar os parâmetros da distribuição a posteriori, a qual já vimos ser Normal-Gama. Agora, perceba que Valciclei fez suas estatística baseado em dados de dez pamonhas, ou seja, $n = 10$. Além disso, as estatísticas que Valciclei conseguiu anotar antes de fazer sua visita de emergência ao banheiro foram, justamente, a média amostral $\left(\overline{x}_n\right)$ e a variância amostral $\left(\overline{s}_n^2\right)$.

Agora, precisamos apenas verificar quais são os hiperparâmetros para nossa priori. Note que Palmirinha suspeita que a concentração de amido está em torno de $10$ mg/L, ou seja, podemos tomar $\mu_0 = 10$. De forma similar a que DeGroot fez em seu exemplo 8.6.3 \cite{degroot}, vamos dividir a variância amostral entre $\mu$ e $\tau$, dessa forma
\[Var(\mu) = \dfrac{\overline{s}_n^2}{2} = 3.965226 \text{ e } E[\tau] = \dfrac{1}{\frac{\overline{s}_n^2}{2}} = \dfrac{1}{3.965226}.\]

Palmirinha também suspeita que o coeficiente de variação da concentração de amido fique em torno de $\frac{1}{2}$, mas note que, em termos de precisão, temos
\[\operatorname{cv} = \dfrac{\sigma}{\mu} = \dfrac{1}{\mu \sqrt{\tau}},\]

\noindent assim, tomando o valor esperado das prioris de $\mu$ e $\tau$, obtemos
\[\operatorname{cv} = \dfrac{1}{\mu_0 \sqrt{\frac{\alpha_0}{\beta_0}}}, \text{ com } \alpha_0 > 1.\]

A condição $\alpha_0 > 1$ é dada para que possamos utilizar o teorema 8.6.3 do DeGroot \cite{degroot}, o qual afirma que
\[Var(\mu) = \dfrac{\beta_0}{\lambda_0 \left(\alpha_0 - 1\right)} \implies \lambda_0 = \dfrac{\beta_0}{Var(\mu) \left(\alpha_0 - 1\right)}.\]

Tomando $\alpha_0 = 2$ podemos encontrar $\beta_0$:
\begin{equation*}
    \begin{split}
        \dfrac{1}{2} & = \dfrac{1}{\mu_0 \sqrt{\frac{\alpha_0}{\beta_0}}} \\
        2 & = 10 \sqrt{\frac{2}{\beta_0}} \\
        \dfrac{1}{5} & = \sqrt{\frac{2}{\beta_0}} \\
        \dfrac{1}{25} & = \dfrac{2}{\beta_0} \\
        \beta_0 & = 50.
    \end{split}
\end{equation*}

Agora, podemos encontrar $\lambda_0$:
\begin{equation*}
    \begin{split}
        \lambda_0 & = \dfrac{50}{3.965226 \left(2 - 1\right)} \\
        & = \dfrac{50}{3.965226}.
    \end{split}
\end{equation*}

Agora, com todos os hiperparâmetros em mãos, podemos fazer o cálculo da posteriori, ou seja, atualizar os hiperparâmetros. Assim, temos
\begin{equation*}
    \begin{split}
        \lambda_1 & = \dfrac{50}{3.965226} + 10 \approx 22.609622 \\
        \mu_1 & = \dfrac{\dfrac{50}{3.965226}\cdot 10 + 10\cdot 8.307849}{10 + \dfrac{50}{3.965226}} \approx 9.251579 \\
        \alpha_1 & = 2 + \dfrac{10}{2} = 7 \\
        \beta_1 & = 50 + \dfrac{7.930452}{2} + \dfrac{10\cdot \dfrac{50}{3.965226}\left(8.307849 - 10\right)^2}{2\left(\dfrac{50}{3.965226} + 10\right)} \approx 61.949896.
    \end{split}
\end{equation*}

Assim, utilizando o último dos resultados acima, temos que
\[\mu \sim X = 9.251579 + \left(\dfrac{61.949896}{7\cdot 22.609622}\right)^{\frac{1}{2}}T,\]

\noindent onde $T$ tem distribuição $t$ de Student com $14$ graus de liberdade.

Assim, utilizando tabelas da distribuição $t$ de Student, podemos ver que o local onde a distribuição deixa probabilidade $0.025$ à direita é o ponto $2.145$, dessa forma, temos que
\[P\left(9.251579 + \left(\dfrac{61.949896}{7\cdot 22.609622}\right)^{\frac{1}{2}}\cdot (-2.145) < \mu < 9.251579 + \left(\dfrac{61.949896}{7\cdot 22.609622}\right)^{\frac{1}{2}}\cdot 2.145\right) = 0.95,\]

\noindent ou seja,
\[P\left(7.909581 < \mu < 10.59358\right) = 0.95.\]

Logo, $a = 7.909581$ e $b = 10.59358$.

Graficamente, temos as seguintes distribuições $\mu$

\begin{figure}[H]
    \begin{tikzpicture}
        \begin{axis}[no markers, domain = 0:18, samples = 300, height = 10 cm, width = 16 cm, enlargelimits = upper, axis on top]
            \addplot [domain = 10.59358:18, draw = black, samples = 100] {studentpost(14)} \closedcycle;
            \addplot [domain = 0:7.9095811, draw = black, samples = 100] {studentpost(14)} \closedcycle;
            \addplot [domain = 7.9095811:10.59358, draw = black, samples = 100] {studentpost(14)} \closedcycle;
            \addplot [draw = blue] {studentpost(14)}
            node [pos=0.45, anchor=mid east, xshift=-2em,
            append after command={
                (\tikzlastnode.east) edge [thin, gray] +(2em,0)}]
            {Posteriori};
            \addplot [draw = cyan] {studentprio(4)}
            node [pos=0.55, anchor=mid west, xshift=2em,
            append after command={
                (\tikzlastnode.west) edge [thin, gray] +(-2em,0)}]
            {Priori};
            \addplot [domain = 7.9095811:10.59358, draw = red, samples = 2, line width=0.1cm] {0}
            node [pos=0.55, anchor=north, yshift=2em] {I.C.};
        \end{axis}
    \end{tikzpicture}
\end{figure}


% \section*{Conclusão}



\printbibliography

\end{document}